## Nano-vLLM-Ascend

nano-vllmæ˜¯å¤–ç½‘å¼€æºçš„ä¸€ä¸ªgpuæ¨ç†é¡¹ç›®ï¼ŒåŸºäºå¼€æºç‰ˆæœ¬å¼„çš„ä¸€ä¸ªascend npuç‰ˆæœ¬æ¨ç†å°demoï¼Œæ—¨åœ¨å¸®åŠ©åˆå­¦è€…äº†è§£æ¨ç†çš„æ•´ä½“æµç¨‹ï¼ŒåŒºåˆ«äºvllmï¼Œnano-vllmä½“é‡æ›´å°ï¼Œéº»é›€è™½å°äº”è„ä¿±å…¨ï¼Œæ›´æœ‰åŠ©äºåˆå­¦è€…å­¦ä¹ ï¼Œéå¸¸é€‚åˆç”¨äºç›¸å…³æ¦‚å¿µçš„ç†è§£ã€‚

## æ¡†æ¶å±‚æµç¨‹å›¾
![alt text](assets/æ¡†æ¶æµç¨‹å›¾.png)
## æ¨¡å‹å±‚æµç¨‹å›¾
![alt text](assets/Qwen3-0.6B.png)

## ç‰¹æ€§
* ğŸ“– **å¯è¯»ä»£ç åº“** - çº¦1200è¡ŒPythonä»£ç çš„æ¸…æ™°å®ç°
* âš¡ **ä¼˜åŒ–å¥—ä»¶** - å¼ é‡å¹¶è¡Œã€torchair Ascend IRå›¾ç¼–è¯‘å’Œå›¾ç¼“å­˜ã€èåˆç®—å­ã€å‰ç¼€ç¼“å­˜ç­‰
- [âœ…] å¾…å®Œæˆï¼šç›®å‰åªæ”¯æŒå•ç®—å­, npuå›¾æ¨¡å¼å®ç°
- [âœ…] æ”¯æŒCPUç¯å¢ƒè¿è¡Œï¼š[nano-vllm-cpu ä»£ç ä»“åº“](https://github.com/linzm1007/nano-vllm-cpu)
- [âœ…] æ€§èƒ½ä¼˜åŒ–
- [â³] æ”¯æŒæ¨¡å‹: Qwen3-0.6Bã€Qwen3-32Bã€Qwen2-0.5Bã€Qwen2.5-0.5Bã€Qwen2.5-0.5B-Instructã€Llama-3.2-1B-Instructã€Qwen3-30B-A3Bã€Qwen3-VL-2B-Instructã€MiniCPM4-0.5B
- [âœ…] æ”¯æŒä¸€ä¸ªmoeæ¨¡å‹:Qwen3-30B-A3B(æš‚æ—¶ä¸æ”¯æŒå…¥å›¾)
- [ğŸ“…] æ”¯æŒä¸€ä¸ªomniæ¨¡å‹
- [âœ…] æ”¯æŒä¸€ä¸ªvlæ¨¡å‹:Qwen3-VL-2B-Instruct(æš‚æ—¶ä¸æ”¯æŒå…¥å›¾)
- [ğŸ“…] å®ç°page attention
- [ğŸ“…] å®ç°ä¸€ä¸ªè‡ªå®šä¹‰ç®—å­
- [ğŸ“…] æ”¯æŒåœ¨çº¿æ¨ç†

torchairæ¥å£å‚è€ƒ https://www.hiascend.com/document/detail/zh/Pytorch/710/modthirdparty/torchairuseguide/torchair_00008.html
èåˆç®—å­æ¥å£å‚è€ƒ https://www.hiascend.com/document/detail/zh/Pytorch/720/apiref/torchnpuCustomsapi/context/torch_npu-npu_fused_infer_attention_score_v2.md
attentionå®ç°å‚è€ƒ https://gitee.com/omniai/omniinfer/blob/master/omni/layers/attention/backend/attention.py  forward_vanillaå‡½æ•°

## æ”¯æŒçš„æ¨¡å‹
| æ¶æ„                   | æ¨¡å‹                    | ç¤ºä¾‹ HF æ¨¡å‹ |
|----------------------|-----------------------|----------|
| Qwen3ForCausalLM     | Qwen3-0.6B,Qwen3-32B  |          |
| Qwen2ForCausalLM     | Qwen2-0.5B            |          |
| LlamaForCausalLM     | Llama-3.2-1B-Instruct |          |
| Qwen3MoeForCausalLM  | Qwen3-30B-A3B         |          |
| Qwen3VLForConditionalGeneration | Qwen2.5-VL-3B-Instruct |          |
| MiniCPMForCausalLM   | MiniCPM4-0.5B         |          |

## benchæ•°æ®
ä»…ä¾›å‚è€ƒï¼Œç¡¬è½¯æ¡ä»¶ä¸åŒï¼Œè·‘å‡ºçš„æ•°æ®ä¹Ÿä¼šæœ‰å·®å¼‚
#### ä¸åŒæ¨¡å‹å¯¹æ¯”
| model                 | Output Tokens | Time (s) | Throughput (tokens/s) | TP |
|-----------------------|---------------|----------|-----------------------|----|
| Qwen3-0.6B            | 143,770       | 36.82    | 3904.20               | 1  |
| Qwen2-0.5B            | 143,770       | 20.71    | 6940.84               | 1  |
| Qwen2.5-0.5B-Instruct | 143,770       | 19.82    | 7252.67               | 1  |
| Llama-3.2-1B-Instruct | 143,770       | 25.45    | 5648.50               | 1  |
| Qwen3-32B             | 143,770       | 206.69   | 695.59               | 2  |
| Qwen3-32B             | 143,770       | 119.86   | 1199.50               | 4  |


#### å…¶ä»–æ¡†æ¶å¯¹æ¯”(2025-12-30)
vLLM Nano-vLLM æ•°æ®æ¥æº https://github.com/GeeeekExplorer/nano-vllm

| Inference Engine                  | Output Tokens | Time (s) | Throughput (tokens/s) |
|-----------------------------------|-------------|----------|-----------------------|
| vLLM                              | 133,966     | 98.37    | 1361.84               |
| Nano-vLLM                         | 133,966     | 93.41    | 1434.13               |
| Nano-vLLM-Ascend python torchåŸç”Ÿå®ç° | 4805     | 257.49    | 18.66               | 
| Nano-vLLM-Ascend èåˆç®—å­+å›¾ç¼–è¯‘bs=256   | 133,966  | 33.88    | 3954.20               |

#### å›¾æ¨¡å¼ä¸åŒbså¯¹æ¯”(2025-12-30)

| Batch Size | Output Tokens | Time (s) | Throughput (tokens/s) |
|------------|---------------|----------|-----------------------|
| bs=16      | 133,966       | 107.23   | 1249.37               |
| bs=32      | 133,966       | 75.89    | 1765.35               |
| bs=48      | 133,966       | 64.84    | 2066.22               |
| bs=64      | 133,966       | 54.06    | 2478.31               |
| bs=128     | 133,966       | 43.08    | 3109.56               |
| bs=256     | 133,966       | 33.88    | 3954.20               |

#### å•ç®—å­Paddingå’ŒNon-paddingå¯¹æ¯”(2025-12-30)
bs=256

| Prepare Strategy | Output Tokens | Time (s) | Throughput (tokens/s) |
|------------------|---------------|----------|-----------------------|
| Padding          | 133,966       | 158.46   | 845.41                |
| Non-padding      | 133,966       | 152.14   | 880.55                |

## ç¯å¢ƒï¼ˆå‚è€ƒvllm-ascendï¼‰
https://docs.vllm.ai/projects/vllm-ascend-cn/zh-cn/latest/quick_start.html

ubuntu
```
# Update DEVICE according to your device (/dev/davinci[0-7])
export DEVICE=/dev/davinci0
# Update the vllm-ascend image
# Atlas A2:
# export IMAGE=quay.io/ascend/vllm-ascend:v0.14.0rc1
# Atlas A3:
# export IMAGE=quay.io/ascend/vllm-ascend:v0.14.0rc1-a3
export IMAGE=quay.io/ascend/vllm-ascend:v0.14.0rc1
docker run --rm \
--name vllm-ascend \
--shm-size=1g \
--device $DEVICE \
--device /dev/davinci_manager \
--device /dev/devmm_svm \
--device /dev/hisi_hdc \
-v /usr/local/dcmi:/usr/local/dcmi \
-v /usr/local/bin/npu-smi:/usr/local/bin/npu-smi \
-v /usr/local/Ascend/driver/lib64/:/usr/local/Ascend/driver/lib64/ \
-v /usr/local/Ascend/driver/version.info:/usr/local/Ascend/driver/version.info \
-v /etc/ascend_install.info:/etc/ascend_install.info \
-v /root/.cache:/root/.cache \
-p 8000:8000 \
-it $IMAGE bash
# Install curl
apt-get update -y && apt-get install -y curl
```

## å®‰è£…ä¾èµ–
```bash
pip install .
```

## æ¨¡å‹ä¸‹è½½

```bash
huggingface-cli download --resume-download Qwen/Qwen3-0.6B \
  --local-dir ~/huggingface/Qwen3-0.6B/ \
  --local-dir-use-symlinks False
```

## å¿«é€Ÿå¼€å§‹

è¯·å‚è§ example.py äº†è§£ç”¨æ³•ã€‚è¯¥ API ä¸ vLLM çš„æ¥å£åŸºæœ¬ä¸€è‡´ï¼Œä»…åœ¨ LLM.generate æ–¹æ³•ä¸Šå­˜åœ¨ä¸€äº›ç»†å¾®å·®å¼‚ï¼š
```python
from nanovllm import LLM, SamplingParams
llm = LLM("/YOUR/MODEL/PATH", enforce_eager=True, tensor_parallel_size=1)
sampling_params = SamplingParams(temperature=0.6, max_tokens=256)
prompts = ["Hello, Nano-vLLM."]
outputs = llm.generate(prompts, sampling_params)
outputs[0]["text"]
```

## exampleè¿è¡Œç»“æœ
![alt text](assets/result-image.png)

## ç¯å¢ƒ
ä»…ä¾›å‚è€ƒ
ascend-dmi -c #æŸ¥çœ‹
* ç¡¬ä»¶ç¯å¢ƒâ€‹ï¼š
  * 1.æ˜¾å¡:A3 910C
  * 2.é©±åŠ¨ç‰ˆæœ¬:24.1.rc3.10
  * 3.å›ºä»¶ç‰ˆæœ¬:7.5.0.109.220
* â€‹è½¯ä»¶ç¯å¢ƒâ€‹ï¼š
  * 1.CANNåŒ… 8.3.RC1
  * 2.PTAç‰ˆæœ¬ï¼štorch-npu 2.5.1.post2+gitd7a85f8ï¼Œtorch 2.5.1

## Benchmark

See `bench.py` for benchmark.

**Test Configuration:**
- Model: Qwen3-0.6B
- Total Requests: 256 sequences
- Input Length: Randomly sampled between 100â€“1024 tokens
- Output Length: Randomly sampled between 100â€“1024 tokens

**Performance Results:**
Nano-vLLM-Ascend å®åœ¨å¤ªæ…¢äº†åªè·‘äº†10æ¡seq

| Inference Engine | Output Tokens | Time (s) | Throughput (tokens/s) |
|----------------|-------------|----------|-----------------------|
| vLLM           | 133,966     | 98.37    | 1361.84               |
| Nano-vLLM      | 133,966     | 93.41    | 1434.13               |
| Nano-vLLM-Ascend| 4805     | 257.49    | 18.66               |



## qwen3-0.6B layers
```angular2html
ModuleList(
  (0-27): 28 x Qwen3DecoderLayer(
    (self_attn): Qwen3Attention(
      (qkv_proj): QKVParallelLinear()
      (o_proj): RowParallelLinear()
      (rotary_emb): RotaryEmbedding()
      (attn): Attention()
      (q_norm): RMSNorm()
      (k_norm): RMSNorm()
    )
    (mlp): Qwen3MLP(
      (gate_up_proj): MergedColumnParallelLinear()
      (down_proj): RowParallelLinear()
      (act_fn): SiluAndMul()
    )
    (input_layernorm): RMSNorm()
    (post_attention_layernorm): RMSNorm()
  )
)

```
